{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSVD_tensorflow_baseline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tea596933/DeepLearningwithPython/blob/master/MSVD_tensorflow_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4JLIuTNbdHN8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MLDS hw2: Video Captioning by RNN"
      ]
    },
    {
      "metadata": {
        "id": "eCwoWFyEdHOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set up connection from colab to my Google Drive."
      ]
    },
    {
      "metadata": {
        "id": "A4WJmJmsdNQ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive ## Have to install for every colab session\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MUCalfGsdHN-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Upload data (MSVD dataset) to colab from my Google Drive, and extract it."
      ]
    },
    {
      "metadata": {
        "id": "OQB-lIvddVEv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "tar_import = drive.CreateFile({'id':'1CBPoo486Rphn2j79B_7HHonwQ--RBAqD'})\n",
        "tar_import.GetContentFile('MSVD.tar')\n",
        "\n",
        "tar = tarfile.open('MSVD.tar')\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMv8e0jPo2Il",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check whether data has been uploaded successfully\n",
        "import os\n",
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSMq25vBdHOa",
        "colab_type": "code",
        "outputId": "c894f171-db7f-4c8b-ab98-01af3c02e81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "m5wRAJEdtGsu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1. Load & Preprocess dataset\n",
        "1. Dataset: MSVD \n",
        "  - 1450 videos for training\n",
        "  - 100 videos for testing\n",
        "  - Each video input has been transformed into frame embeddings (80 frames * 4096) by VGG-19.\n",
        "  \n",
        "2. Preprocess label (Sentences)\n",
        "  - Each video has several different descriptions in this dataset.\n",
        "  - Discard words with text frequency <= 3, following the offered baseline of the homework slide.\n",
        "  - Pad word sentences to the same length ```maxlen``` (20) using word index ```1```.\n",
        "  - Words will be indexed larger than 1. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SYJwHXh7dHOg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#vocab_size = 6016  # For debugging. Comment it when finish debugging\n",
        "\n",
        "EOS = 1\n",
        "input_embedding_size = 4096\n",
        "encoder_hidden_units = 100\n",
        "decoder_hidden_units = 100\n",
        "batchSize = 64\n",
        "maxlen = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V5DWzbRdciGU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(dtrain_label_word, wordCnt_lowerBdd=3):\n",
        "    wordList = {}\n",
        "    distinctWordCnt = 0\n",
        "    tokenized_sentences = []\n",
        "    existSet = set()\n",
        "    \n",
        "    for word_seq in dtrain_label_word:\n",
        "        word_seq_ = re.sub('[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', word_seq)\n",
        "        word_seq_ = word_seq_.lower()\n",
        "        split_words = word_seq_.split(' ')\n",
        "        for word in split_words:\n",
        "            if word in existSet:\n",
        "                wordList[word]['count'] += 1\n",
        "            else:\n",
        "                distinctWordCnt += 1\n",
        "                wordList[word] = {\n",
        "                    'count': 1,\n",
        "                    'index': distinctWordCnt + 1\n",
        "                }\n",
        "                existSet.add(word)\n",
        "        tokenized_sentences.append(split_words)\n",
        "\n",
        "    wordList = {key: value['index'] for key, value in wordList.items() if value['count'] > wordCnt_lowerBdd}\n",
        "    existSet = set([key for key,_ in wordList.items()])\n",
        "\n",
        "    wordList_ordered = {word: (idx+2) for idx, (word,_) in enumerate(wordList.items())}        \n",
        "        \n",
        "    for iWord_seq in range(len(tokenized_sentences)):\n",
        "        dataIn = []\n",
        "        for word in tokenized_sentences[iWord_seq]:\n",
        "            if word in existSet:\n",
        "                dataIn.append(wordList_ordered[word])\n",
        "        tokenized_sentences[iWord_seq] = dataIn\n",
        "        \n",
        "    \n",
        "    return tokenized_sentences, wordList_ordered, existSet\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RjMdHur-dXZw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dtrain_feat_path = os.listdir('MSVD/training_data/feat')\n",
        "dtrain_feat_raw = []\n",
        "for path in dtrain_feat_path:\n",
        "    dataIn = np.load('MSVD/training_data/feat/'+path)\n",
        "    dtrain_feat_raw.append(dataIn)\n",
        "dtrain_label_raw = pd.read_json('MSVD/training_label.json')\n",
        "\n",
        "dtrain_name = [re.sub('.npy$','',x) for x in dtrain_feat_path]\n",
        "\n",
        "dtrain_feat = []\n",
        "dtrain_label_word = []\n",
        "for i, feat in enumerate(dtrain_feat_raw):\n",
        "    filename = dtrain_name[i]\n",
        "    labels = dtrain_label_raw.loc[dtrain_label_raw.id==filename].caption.iloc[0]\n",
        "    for label in labels:\n",
        "        dtrain_feat.append(feat)\n",
        "        dtrain_label_word.append(label)\n",
        "\n",
        "dtrain_label, wordList, existSet = tokenize(dtrain_label_word, 3)        \n",
        "        \n",
        "vocab_size = len(existSet)\n",
        "dtrain_label = pad_sequences(dtrain_label, \n",
        "                             maxlen=maxlen, \n",
        "                             padding='post',\n",
        "                             value=EOS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wcmDOWyxdHOk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2. Build TensorFlow Model (Graph)\n",
        "Use LSTM encoder & LSTM decoder to build the following model.\n",
        "1. Encoder Setting\n",
        "  - ```encoder_inputs```: batch of embeddings of video frames (from ```dtrain_feat```)\n",
        "  - ```encoder_cell```: LSTM cell with #units = ```encoder_hidden_units```\n",
        "  - ```encoder_state```: init LSTM state with zeros.\n",
        "  \n",
        "2. Decoder Setting\n",
        "  - ```decoder_inputs```: batch of sentences (from ```dtrain_label```) start from 0, meaning start of sentence.\n",
        "  - ```decoder_targets```: batch of sentences (from ```dtrain_label```) start from the first word of each sentence.\n",
        "  - ```decoder_input_oh``` & ```decoder_output_oh```: transform word index(e.g. ```1```) into one-hot encoding(e.g. ```[0,1,0,0,...]```).\n",
        "  - ```decoder_targets```: batch of sentences (from ```dtrain_label```) start from the first word of each sentence.\n",
        "  - ```encoder_cell```: LSTM cell with #units = ```encoder_hidden_units```\n",
        "  - ```encoder_state```: init LSTM state with zeros.\n",
        "\n",
        "3. "
      ]
    },
    {
      "metadata": {
        "id": "FpPd9EyvdHOk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_inputs = tf.placeholder(shape=(None, 80, input_embedding_size), dtype=tf.float32, name='encoder_inputs')\n",
        "decoder_inputs = tf.placeholder(shape=(None, maxlen), dtype=tf.int32, name='decoder_inputs')\n",
        "decoder_targets = tf.placeholder(shape=(None, maxlen), dtype=tf.int32, name='decoder_targets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EDQAc0DzdHO2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "\n",
        "The centerpiece of all things RNN in TensorFlow is `RNNCell` class and its descendants (like `LSTMCell`). But they are outside of the scope of this post — nice [official tutorial](https://www.tensorflow.org/tutorials/recurrent/) is available. \n",
        "\n",
        "`@TODO: RNNCell as a factory`"
      ]
    },
    {
      "metadata": {
        "id": "5ny5mMMEdHO3",
        "colab_type": "code",
        "outputId": "772e941c-e841-45a6-a712-fba8e8b65160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units, state_is_tuple=False)\n",
        "#print(encoder_cell.state_size)\n",
        "#init_state = tf.placeholder(shape=(None, encoder_cell.state_size), dtype=tf.float32, name='init_state')\n",
        "encoder_state = tf.zeros([batchSize, encoder_cell.state_size], dtype=tf.float32)\n",
        "\n",
        "for i in range(80):\n",
        "    with tf.variable_scope(\"encoder\",reuse=(i!=0)):\n",
        "        encoder_outputs, encoder_state = encoder_cell(encoder_inputs[:,i,:], encoder_state)\n",
        "\n",
        "#del encoder_outputs\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9db8ad8a20>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r5Tjl-mzdHO8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorFlow LSTM implementation stores state as a tuple of tensors. \n",
        "- `encoder_final_state.h` is activations of hidden layer of LSTM cell\n",
        "- `encoder_final_state.c` is final output, which can potentially be transfromed with some wrapper `@TODO: check correctness`"
      ]
    },
    {
      "metadata": {
        "id": "uhC8s9DZdHO8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ]
    },
    {
      "metadata": {
        "id": "dvgNgmyCoH0W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_input_oh = tf.one_hot(decoder_inputs, vocab_size+2, on_value=1.0, off_value=0.0, axis=-1)\n",
        "#decoder_input_oh.shape\n",
        "decoder_target_oh = tf.one_hot(decoder_targets, vocab_size+2, on_value=1.0, off_value=0.0, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5nXJvWljdHO9",
        "colab_type": "code",
        "outputId": "ecc0706b-7b50-40d0-f1fc-6d5ea738e716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "cell_type": "code",
      "source": [
        "decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units, state_is_tuple=False)\n",
        "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size+2], -1, 1), dtype=tf.float32)\n",
        "b = tf.Variable(tf.zeros([vocab_size+2]), dtype=tf.float32)\n",
        "\n",
        "embed_w = tf.Variable(tf.random_uniform([vocab_size+2, decoder_hidden_units], -1, 1), dtype=tf.float32)\n",
        "embed_b  = tf.Variable(tf.zeros([decoder_hidden_units]), dtype=tf.float32)\n",
        "\n",
        "pred_probs = []\n",
        "loss = 0.0\n",
        "\n",
        "decoder_state = encoder_state\n",
        "print('encoder state shape: ', encoder_state.shape)\n",
        "print('decoder state shape: ',decoder_state.shape)\n",
        "\n",
        "\n",
        "for i in range(maxlen):\n",
        "    decoder_input_embed = tf.nn.xw_plus_b(decoder_input_oh[:,i,:], embed_w, embed_b)\n",
        "\n",
        "    with tf.variable_scope(\"decoder\",reuse=(i!=0)):\n",
        "        #decoder_output, decoder_state = decoder_cell(decoder_input_oh[:,i,:], decoder_state)\n",
        "        decoder_output, decoder_state = decoder_cell(decoder_input_embed, decoder_state)\n",
        "        ### calculate loss ### \n",
        "        # create answer \n",
        "        answer_in_onehot = decoder_target_oh[:,i]\n",
        "#         answer_index_in_vocab = decoder_targets[:,i]\n",
        "#         answer_index_in_vocab = tf.expand_dims(answer_index_in_vocab, 1)\n",
        "#         batch_index\t= tf.expand_dims(tf.range(0, batchSize, 1), 1)\n",
        "#         sparse_mat \t= tf.concat([batch_index, answer_index_in_vocab], 1)\n",
        "#         answer_in_onehot = tf.sparse_to_dense(sparse_mat, tf.stack([batchSize, vocab_size+2]), 1.0, 0.0)\n",
        "\n",
        "        # acquire output\n",
        "        logits = tf.nn.xw_plus_b(decoder_output, W, b)\n",
        "        decoder_prediction = tf.argmax(logits, 1)\n",
        "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=answer_in_onehot)\n",
        "        #cross_entropy = cross_entropy * self.caption_mask[:,i]\n",
        "\n",
        "    pred_probs.append(logits)\n",
        "    this_loss = tf.reduce_mean(cross_entropy)\n",
        "    loss = loss + this_loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9db3571780>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
            "encoder state shape:  (64, 200)\n",
            "decoder state shape:  (64, 200)\n",
            "WARNING:tensorflow:From <ipython-input-11-1a247b7e1333>:34: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x0dbit9LdHPC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "6-fDKxaedHPD",
        "colab_type": "code",
        "outputId": "eac95ee5-8fd5-40f3-8ec8-a35b971b38d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'add_19:0' shape=() dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "5gXObnKqdHPF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RNN outputs tensor of shape `[max_time, batch_size, hidden_units]` which projection layer maps onto `[max_time, batch_size, vocab_size]`. `vocab_size` part of the shape is static, while `max_time` and `batch_size` is dynamic."
      ]
    },
    {
      "metadata": {
        "id": "epOvYmCzdHPF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_op = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BfdxjtMxdHPJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14D5ykoZdHPL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test forward pass\n",
        "\n",
        "Did I say that deep learning is a game of shapes? When building a Graph, TF will throw errors when static shapes are not matching. However, mismatches between dynamic shapes are often only discovered when we try to run something through the graph.\n",
        "\n",
        "\n",
        "So let's try running something. For that we need to prepare values we will feed into placeholders."
      ]
    },
    {
      "metadata": {
        "id": "FAGWUwSxi4mm",
        "colab_type": "code",
        "outputId": "480db36e-a03c-4135-d18a-97583d11803f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "rand_inds = random.sample(range(len(dtrain_feat)), batchSize)\n",
        "encoder_input_data = [dtrain_feat[i] for i in rand_inds]\n",
        "encoder_input_data = np.array(encoder_input_data, dtype='float32')\n",
        "            \n",
        "din_ = np.zeros(shape=(batchSize, maxlen), dtype=np.int32)\n",
        "\n",
        "pred_ = sess.run(logits,\n",
        "    feed_dict={\n",
        "        encoder_inputs: encoder_input_data,\n",
        "        decoder_inputs: din_,\n",
        "        decoder_targets: din_\n",
        "    })\n",
        "print('decoder predictions:\\n' + str(pred_))\n",
        "print(pred_.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder predictions:\n",
            "[[-0.19776438  0.29026493  0.12567572 ...  0.04694255 -0.12126266\n",
            "  -0.14837316]\n",
            " [-0.19768818  0.29023704  0.12576102 ...  0.04698886 -0.12125028\n",
            "  -0.14843422]\n",
            " [-0.1977036   0.29024327  0.12570518 ...  0.04695785 -0.12126548\n",
            "  -0.14843154]\n",
            " ...\n",
            " [-0.19775361  0.29023373  0.12569948 ...  0.04686578 -0.12129899\n",
            "  -0.14832814]\n",
            " [-0.19769567  0.2902351   0.12563488 ...  0.04693524 -0.12134545\n",
            "  -0.14843549]\n",
            " [-0.19774145  0.29022014  0.12571456 ...  0.04694584 -0.12128685\n",
            "  -0.14840095]]\n",
            "(64, 6018)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W6GyaTGbdHPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Successful forward computation, everything is wired correctly."
      ]
    },
    {
      "metadata": {
        "id": "h8ihLM9HdHPP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training on the toy task"
      ]
    },
    {
      "metadata": {
        "id": "y5AxUuJtdHPR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will teach our model to memorize and reproduce input sequence. Sequences will be random, with varying length.\n",
        "\n",
        "Since random sequences do not contain any structure, model will not be able to exploit any patterns in data. It will simply encode sequence in a thought vector, then decode from it."
      ]
    },
    {
      "metadata": {
        "id": "7UsfY97NdHPV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_feed():\n",
        "    rand_inds = random.sample(range(len(dtrain_feat)), batchSize)\n",
        "    encoder_input_data = [dtrain_feat[i] for i in rand_inds]\n",
        "    encoder_input_data = np.array(encoder_input_data, dtype='float32')\n",
        "    decoder_input_data = np.zeros((batchSize, maxlen), dtype='float32')\n",
        "    decoder_target_data = np.zeros((batchSize, maxlen), dtype='float32')\n",
        "\n",
        "    dtrain_label_batch = [dtrain_label[i] for i in rand_inds]        \n",
        "    for i, target_text in enumerate(dtrain_label_batch):\n",
        "        for t, word_index in enumerate(target_text):\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t] = word_index\n",
        "            if t < len(target_text)-1:\n",
        "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "                decoder_input_data[i, t + 1] = word_index\n",
        "\n",
        "    encoder_inputs_ = encoder_input_data\n",
        "    decoder_targets_ = decoder_target_data\n",
        "    decoder_inputs_ = decoder_input_data\n",
        "        \n",
        "    return {\n",
        "        encoder_inputs: encoder_inputs_,\n",
        "        decoder_inputs: decoder_inputs_,\n",
        "        decoder_targets: decoder_targets_\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9U2t6_zxdHPY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_track = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8O_S6P8ddHPa",
        "colab_type": "code",
        "outputId": "e250d528-3bd1-4a9a-c52b-8490a4ae802f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5201
        }
      },
      "cell_type": "code",
      "source": [
        "batches_in_epoch = (len(dtrain_label) // 64) + 1\n",
        "#epochs = 5\n",
        "epochs = 100\n",
        "max_batches = batches_in_epoch * epochs\n",
        "epochCnt = 0\n",
        "\n",
        "try:\n",
        "    for batch in range(max_batches):\n",
        "        fd = next_feed()\n",
        "        _, l = sess.run([train_op, loss], fd)\n",
        "        loss_track.append(l)\n",
        "\n",
        "        if batch == 0 or batch % batches_in_epoch == 0:\n",
        "            #print('batch {}'.format(batch))\n",
        "            print('epoch {}'.format(epochCnt))\n",
        "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
        "            #predict_ = sess.run(decoder_prediction, fd)\n",
        "            #for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
        "                #print('  sample {}:'.format(i + 1))\n",
        "                #print('    input     > {}'.format(inp))\n",
        "                #print('    predicted > {}'.format(pred))\n",
        "                #if i >= 2:\n",
        "                    #break\n",
        "            print()\n",
        "            epochCnt += 1\n",
        "except KeyboardInterrupt:\n",
        "    print('training interrupted')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "  minibatch loss: 118.87995147705078\n",
            "\n",
            "epoch 1\n",
            "  minibatch loss: 28.304059982299805\n",
            "\n",
            "epoch 2\n",
            "  minibatch loss: 24.055269241333008\n",
            "\n",
            "epoch 3\n",
            "  minibatch loss: 21.749479293823242\n",
            "\n",
            "epoch 4\n",
            "  minibatch loss: 19.73598861694336\n",
            "\n",
            "epoch 5\n",
            "  minibatch loss: 16.214265823364258\n",
            "\n",
            "epoch 6\n",
            "  minibatch loss: 16.377849578857422\n",
            "\n",
            "epoch 7\n",
            "  minibatch loss: 15.191335678100586\n",
            "\n",
            "epoch 8\n",
            "  minibatch loss: 13.078781127929688\n",
            "\n",
            "epoch 9\n",
            "  minibatch loss: 13.027229309082031\n",
            "\n",
            "epoch 10\n",
            "  minibatch loss: 15.775493621826172\n",
            "\n",
            "epoch 11\n",
            "  minibatch loss: 12.20848560333252\n",
            "\n",
            "epoch 12\n",
            "  minibatch loss: 14.183123588562012\n",
            "\n",
            "epoch 13\n",
            "  minibatch loss: 12.694012641906738\n",
            "\n",
            "epoch 14\n",
            "  minibatch loss: 13.205793380737305\n",
            "\n",
            "epoch 15\n",
            "  minibatch loss: 13.154470443725586\n",
            "\n",
            "epoch 16\n",
            "  minibatch loss: 9.485584259033203\n",
            "\n",
            "epoch 17\n",
            "  minibatch loss: 11.962868690490723\n",
            "\n",
            "epoch 18\n",
            "  minibatch loss: 10.615108489990234\n",
            "\n",
            "epoch 19\n",
            "  minibatch loss: 9.792715072631836\n",
            "\n",
            "epoch 20\n",
            "  minibatch loss: 13.019918441772461\n",
            "\n",
            "epoch 21\n",
            "  minibatch loss: 10.452183723449707\n",
            "\n",
            "epoch 22\n",
            "  minibatch loss: 11.300663948059082\n",
            "\n",
            "epoch 23\n",
            "  minibatch loss: 8.47718620300293\n",
            "\n",
            "epoch 24\n",
            "  minibatch loss: 8.989986419677734\n",
            "\n",
            "epoch 25\n",
            "  minibatch loss: 11.102269172668457\n",
            "\n",
            "epoch 26\n",
            "  minibatch loss: 9.067713737487793\n",
            "\n",
            "epoch 27\n",
            "  minibatch loss: 8.042531967163086\n",
            "\n",
            "epoch 28\n",
            "  minibatch loss: 8.962018966674805\n",
            "\n",
            "epoch 29\n",
            "  minibatch loss: 7.7012457847595215\n",
            "\n",
            "epoch 30\n",
            "  minibatch loss: 7.728286266326904\n",
            "\n",
            "epoch 31\n",
            "  minibatch loss: 9.596446990966797\n",
            "\n",
            "epoch 32\n",
            "  minibatch loss: 7.943232536315918\n",
            "\n",
            "epoch 33\n",
            "  minibatch loss: 8.90894889831543\n",
            "\n",
            "epoch 34\n",
            "  minibatch loss: 7.923971652984619\n",
            "\n",
            "epoch 35\n",
            "  minibatch loss: 8.06932258605957\n",
            "\n",
            "epoch 36\n",
            "  minibatch loss: 7.981895923614502\n",
            "\n",
            "epoch 37\n",
            "  minibatch loss: 9.466817855834961\n",
            "\n",
            "epoch 38\n",
            "  minibatch loss: 8.212082862854004\n",
            "\n",
            "epoch 39\n",
            "  minibatch loss: 8.264856338500977\n",
            "\n",
            "epoch 40\n",
            "  minibatch loss: 8.45732307434082\n",
            "\n",
            "epoch 41\n",
            "  minibatch loss: 7.739621162414551\n",
            "\n",
            "epoch 42\n",
            "  minibatch loss: 6.894102573394775\n",
            "\n",
            "epoch 43\n",
            "  minibatch loss: 7.24849271774292\n",
            "\n",
            "epoch 44\n",
            "  minibatch loss: 8.229033470153809\n",
            "\n",
            "epoch 45\n",
            "  minibatch loss: 6.477254867553711\n",
            "\n",
            "epoch 46\n",
            "  minibatch loss: 8.42904281616211\n",
            "\n",
            "epoch 47\n",
            "  minibatch loss: 6.242530822753906\n",
            "\n",
            "epoch 48\n",
            "  minibatch loss: 7.17818546295166\n",
            "\n",
            "epoch 49\n",
            "  minibatch loss: 6.916050434112549\n",
            "\n",
            "epoch 50\n",
            "  minibatch loss: 6.02446985244751\n",
            "\n",
            "epoch 51\n",
            "  minibatch loss: 7.5687031745910645\n",
            "\n",
            "epoch 52\n",
            "  minibatch loss: 7.000394344329834\n",
            "\n",
            "epoch 53\n",
            "  minibatch loss: 6.95857572555542\n",
            "\n",
            "epoch 54\n",
            "  minibatch loss: 6.923158645629883\n",
            "\n",
            "epoch 55\n",
            "  minibatch loss: 5.572431564331055\n",
            "\n",
            "epoch 56\n",
            "  minibatch loss: 6.98112678527832\n",
            "\n",
            "epoch 57\n",
            "  minibatch loss: 6.364977836608887\n",
            "\n",
            "epoch 58\n",
            "  minibatch loss: 6.157962799072266\n",
            "\n",
            "epoch 59\n",
            "  minibatch loss: 6.206793785095215\n",
            "\n",
            "epoch 60\n",
            "  minibatch loss: 6.251272201538086\n",
            "\n",
            "epoch 61\n",
            "  minibatch loss: 6.473416328430176\n",
            "\n",
            "epoch 62\n",
            "  minibatch loss: 5.617919921875\n",
            "\n",
            "epoch 63\n",
            "  minibatch loss: 5.644163131713867\n",
            "\n",
            "epoch 64\n",
            "  minibatch loss: 5.900916576385498\n",
            "\n",
            "epoch 65\n",
            "  minibatch loss: 6.177117824554443\n",
            "\n",
            "epoch 66\n",
            "  minibatch loss: 6.015290260314941\n",
            "\n",
            "epoch 67\n",
            "  minibatch loss: 5.361050128936768\n",
            "\n",
            "epoch 68\n",
            "  minibatch loss: 5.36929178237915\n",
            "\n",
            "epoch 69\n",
            "  minibatch loss: 5.347686767578125\n",
            "\n",
            "epoch 70\n",
            "  minibatch loss: 6.01144552230835\n",
            "\n",
            "epoch 71\n",
            "  minibatch loss: 5.322937965393066\n",
            "\n",
            "epoch 72\n",
            "  minibatch loss: 6.1964874267578125\n",
            "\n",
            "epoch 73\n",
            "  minibatch loss: 6.7301926612854\n",
            "\n",
            "epoch 74\n",
            "  minibatch loss: 5.264472007751465\n",
            "\n",
            "epoch 75\n",
            "  minibatch loss: 5.7399725914001465\n",
            "\n",
            "epoch 76\n",
            "  minibatch loss: 4.92203426361084\n",
            "\n",
            "epoch 77\n",
            "  minibatch loss: 5.418150901794434\n",
            "\n",
            "epoch 78\n",
            "  minibatch loss: 5.132318019866943\n",
            "\n",
            "epoch 79\n",
            "  minibatch loss: 4.737917900085449\n",
            "\n",
            "epoch 80\n",
            "  minibatch loss: 5.256088733673096\n",
            "\n",
            "epoch 81\n",
            "  minibatch loss: 6.0444207191467285\n",
            "\n",
            "epoch 82\n",
            "  minibatch loss: 5.226744174957275\n",
            "\n",
            "epoch 83\n",
            "  minibatch loss: 4.699752330780029\n",
            "\n",
            "epoch 84\n",
            "  minibatch loss: 4.891371250152588\n",
            "\n",
            "epoch 85\n",
            "  minibatch loss: 5.093839168548584\n",
            "\n",
            "epoch 86\n",
            "  minibatch loss: 4.774623870849609\n",
            "\n",
            "epoch 87\n",
            "  minibatch loss: 4.278818130493164\n",
            "\n",
            "epoch 88\n",
            "  minibatch loss: 4.168238162994385\n",
            "\n",
            "epoch 89\n",
            "  minibatch loss: 5.082961559295654\n",
            "\n",
            "epoch 90\n",
            "  minibatch loss: 4.619229316711426\n",
            "\n",
            "epoch 91\n",
            "  minibatch loss: 4.639917373657227\n",
            "\n",
            "epoch 92\n",
            "  minibatch loss: 4.749841690063477\n",
            "\n",
            "epoch 93\n",
            "  minibatch loss: 4.941689491271973\n",
            "\n",
            "epoch 94\n",
            "  minibatch loss: 4.39129114151001\n",
            "\n",
            "epoch 95\n",
            "  minibatch loss: 4.903537273406982\n",
            "\n",
            "epoch 96\n",
            "  minibatch loss: 5.037909507751465\n",
            "\n",
            "epoch 97\n",
            "  minibatch loss: 4.417563438415527\n",
            "\n",
            "epoch 98\n",
            "  minibatch loss: 4.495705604553223\n",
            "\n",
            "epoch 99\n",
            "  minibatch loss: 4.3960371017456055\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JAakElPadHPc",
        "colab_type": "code",
        "outputId": "10e6922f-6e1a-4e92-9e88-8c3ee17831c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_track)\n",
        "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batchSize, batchSize))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 4.7188 after 2425600 examples (batch_size=64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nj6jURUTyiEpdRCSPqNRFRPLI/weuAIHMJTTyZgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9e0c7f6080>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HatOzIfZd0A1",
        "colab_type": "code",
        "outputId": "b16898a3-3b08-472a-a525-8a83aa32e8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Later, launch the model, initialize the variables, do some work, and save the\n",
        "# variables to disk.\n",
        "if os.path.exists('model') is False:\n",
        "    os.mkdir('model')\n",
        "save_path = saver.save(sess, \"model/model_embed_100epoch_100.ckpt\")\n",
        "#save_path = saver.save(sess, \"model/model_test.ckpt\")\n",
        "print(\"Model saved in path: %s\" % save_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: model/model_embed_100epoch_100.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_VDAsBHTDLxa",
        "colab_type": "code",
        "outputId": "f32b084d-50e3-4c2b-d78c-3b0225365c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "!tar -jcv -f model_embed_100_100.tar.bz2 model/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model/\n",
            "model/checkpoint\n",
            "model/model_embed_200epoch_256.ckpt.data-00000-of-00001\n",
            "model/model_embed_200epoch_256.ckpt.index\n",
            "model/model_embed_200epoch_256.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BD_L7iQoDzRp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model_embed_100_100.tar.bz2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7UHQ8ba26vXH",
        "colab_type": "code",
        "outputId": "8aa6670f-e645-4c50-89a2-42d45c030f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "index_list = []\n",
        "for char, i in wordList.items():\n",
        "    index_list.append(i)\n",
        "print(index_list)\n",
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 201, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 222, 223, 224, 225, 226, 227, 230, 231, 232, 233, 235, 236, 237, 238, 239, 240, 241, 243, 244, 245, 246, 248, 249, 250, 255, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 285, 286, 290, 291, 292, 293, 294, 295, 296, 297, 298, 301, 303, 306, 307, 309, 310, 312, 314, 315, 316, 317, 319, 323, 324, 325, 326, 327, 328, 330, 331, 332, 333, 334, 335, 336, 337, 340, 341, 342, 343, 344, 345, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 380, 381, 382, 385, 386, 387, 388, 392, 394, 395, 396, 397, 398, 399, 400, 402, 403, 405, 406, 407, 408, 409, 411, 412, 413, 414, 416, 418, 419, 421, 422, 423, 424, 425, 426, 427, 428, 430, 431, 432, 433, 434, 435, 436, 437, 438, 440, 441, 442, 446, 447, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 479, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493, 495, 496, 497, 501, 502, 503, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518, 521, 522, 525, 528, 529, 531, 532, 533, 534, 538, 540, 541, 542, 543, 544, 545, 546, 547, 548, 551, 552, 553, 556, 557, 558, 559, 564, 568, 569, 570, 571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 584, 585, 586, 587, 588, 589, 590, 591, 592, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 607, 610, 611, 613, 614, 615, 616, 617, 618, 619, 620, 621, 623, 624, 625, 626, 629, 631, 632, 633, 634, 635, 637, 640, 642, 643, 645, 649, 650, 653, 654, 655, 656, 658, 659, 661, 663, 669, 671, 672, 673, 674, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 687, 692, 693, 695, 696, 697, 698, 699, 702, 703, 704, 706, 707, 708, 709, 710, 711, 712, 713, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 738, 739, 740, 741, 742, 743, 744, 748, 750, 754, 756, 757, 758, 759, 760, 764, 765, 766, 768, 772, 773, 774, 775, 776, 777, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 797, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 825, 826, 829, 830, 831, 832, 833, 834, 835, 836, 838, 840, 841, 843, 845, 850, 853, 857, 860, 862, 864, 866, 868, 870, 872, 873, 874, 875, 876, 878, 879, 880, 881, 882, 883, 884, 885, 887, 888, 889, 890, 893, 894, 895, 899, 900, 901, 902, 903, 904, 906, 907, 908, 909, 910, 911, 912, 913, 914, 916, 917, 918, 921, 922, 923, 924, 926, 927, 928, 929, 931, 932, 934, 936, 937, 939, 940, 942, 943, 944, 945, 946, 948, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 963, 965, 966, 967, 968, 969, 970, 971, 972, 973, 975, 976, 977, 978, 979, 981, 982, 985, 988, 989, 990, 992, 993, 994, 995, 996, 997, 998, 999, 1001, 1002, 1003, 1005, 1008, 1009, 1010, 1011, 1013, 1014, 1015, 1017, 1020, 1021, 1022, 1024, 1025, 1026, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1038, 1043, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1057, 1058, 1059, 1061, 1062, 1064, 1065, 1066, 1069, 1071, 1074, 1075, 1076, 1078, 1080, 1084, 1087, 1094, 1097, 1100, 1101, 1102, 1106, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1123, 1124, 1125, 1126, 1128, 1131, 1133, 1134, 1136, 1137, 1138, 1139, 1140, 1142, 1145, 1146, 1147, 1149, 1150, 1151, 1153, 1160, 1161, 1164, 1165, 1166, 1167, 1171, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1196, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1209, 1211, 1213, 1214, 1215, 1216, 1217, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1246, 1248, 1249, 1250, 1251, 1252, 1253, 1256, 1257, 1258, 1259, 1262, 1263, 1266, 1267, 1268, 1270, 1271, 1272, 1273, 1275, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1286, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1298, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1318, 1319, 1321, 1322, 1323, 1325, 1329, 1331, 1332, 1337, 1338, 1341, 1342, 1344, 1346, 1347, 1348, 1350, 1351, 1353, 1354, 1355, 1356, 1357, 1359, 1360, 1365, 1366, 1368, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1385, 1386, 1387, 1388, 1389, 1391, 1392, 1393, 1394, 1395, 1398, 1399, 1401, 1402, 1404, 1405, 1412, 1417, 1418, 1419, 1420, 1421, 1423, 1424, 1425, 1426, 1427, 1428, 1431, 1433, 1434, 1435, 1436, 1437, 1439, 1440, 1441, 1442, 1443, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1453, 1454, 1455, 1456, 1457, 1458, 1461, 1464, 1465, 1466, 1469, 1470, 1471, 1473, 1476, 1477, 1478, 1479, 1482, 1483, 1484, 1486, 1487, 1489, 1491, 1492, 1493, 1494, 1496, 1497, 1500, 1502, 1504, 1509, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1525, 1527, 1528, 1529, 1536, 1537, 1539, 1540, 1541, 1543, 1545, 1548, 1549, 1552, 1554, 1555, 1556, 1558, 1560, 1563, 1564, 1565, 1566, 1567, 1571, 1572, 1573, 1575, 1577, 1578, 1581, 1582, 1583, 1586, 1587, 1588, 1589, 1591, 1593, 1594, 1595, 1596, 1603, 1604, 1605, 1606, 1608, 1610, 1611, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1623, 1624, 1625, 1626, 1638, 1640, 1641, 1642, 1644, 1646, 1648, 1650, 1651, 1652, 1653, 1654, 1656, 1657, 1668, 1669, 1670, 1678, 1679, 1680, 1681, 1682, 1683, 1685, 1686, 1688, 1692, 1693, 1695, 1697, 1698, 1699, 1700, 1701, 1702, 1704, 1707, 1712, 1714, 1715, 1722, 1723, 1724, 1725, 1726, 1729, 1730, 1732, 1733, 1735, 1737, 1740, 1741, 1743, 1744, 1745, 1746, 1747, 1748, 1751, 1753, 1756, 1758, 1759, 1761, 1765, 1767, 1768, 1769, 1770, 1771, 1775, 1776, 1777, 1780, 1781, 1784, 1785, 1786, 1788, 1789, 1790, 1791, 1792, 1795, 1796, 1797, 1798, 1799, 1800, 1803, 1805, 1807, 1809, 1810, 1811, 1815, 1816, 1825, 1826, 1829, 1831, 1832, 1833, 1835, 1836, 1838, 1843, 1844, 1845, 1846, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1860, 1864, 1866, 1869, 1870, 1871, 1873, 1874, 1877, 1878, 1879, 1881, 1882, 1888, 1889, 1890, 1893, 1894, 1895, 1897, 1899, 1900, 1903, 1910, 1911, 1912, 1917, 1918, 1919, 1922, 1923, 1925, 1927, 1928, 1932, 1938, 1940, 1941, 1942, 1943, 1944, 1946, 1947, 1949, 1950, 1951, 1952, 1958, 1959, 1962, 1963, 1964, 1965, 1966, 1967, 1969, 1970, 1971, 1974, 1975, 1976, 1977, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1990, 1991, 1993, 1994, 1997, 2000, 2005, 2007, 2008, 2011, 2012, 2014, 2015, 2016, 2018, 2020, 2021, 2028, 2029, 2030, 2032, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2043, 2044, 2045, 2046, 2055, 2059, 2062, 2063, 2067, 2073, 2075, 2077, 2078, 2080, 2082, 2084, 2085, 2086, 2090, 2091, 2092, 2100, 2101, 2103, 2105, 2108, 2110, 2112, 2113, 2118, 2121, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2132, 2133, 2134, 2135, 2136, 2138, 2142, 2143, 2144, 2145, 2147, 2148, 2151, 2153, 2156, 2157, 2158, 2160, 2161, 2164, 2167, 2170, 2174, 2175, 2177, 2178, 2180, 2184, 2185, 2187, 2189, 2192, 2194, 2196, 2197, 2199, 2201, 2202, 2204, 2208, 2209, 2210, 2214, 2216, 2218, 2221, 2223, 2226, 2228, 2234, 2235, 2239, 2240, 2241, 2247, 2250, 2251, 2252, 2253, 2255, 2256, 2257, 2260, 2261, 2262, 2263, 2265, 2266, 2268, 2274, 2277, 2280, 2282, 2293, 2298, 2299, 2301, 2302, 2303, 2304, 2308, 2309, 2310, 2311, 2313, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2326, 2327, 2331, 2340, 2343, 2347, 2348, 2351, 2353, 2359, 2360, 2362, 2363, 2365, 2371, 2372, 2381, 2384, 2389, 2391, 2392, 2394, 2395, 2397, 2398, 2402, 2403, 2407, 2409, 2413, 2414, 2416, 2419, 2421, 2423, 2430, 2434, 2436, 2437, 2443, 2445, 2449, 2452, 2456, 2457, 2462, 2464, 2466, 2468, 2471, 2473, 2475, 2477, 2479, 2482, 2483, 2490, 2493, 2494, 2495, 2496, 2501, 2502, 2503, 2504, 2507, 2512, 2514, 2516, 2517, 2518, 2525, 2526, 2527, 2532, 2533, 2535, 2537, 2538, 2539, 2542, 2544, 2545, 2547, 2548, 2549, 2552, 2555, 2561, 2562, 2563, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2575, 2577, 2585, 2587, 2590, 2591, 2592, 2593, 2594, 2599, 2605, 2607, 2608, 2623, 2626, 2629, 2630, 2633, 2635, 2639, 2640, 2641, 2645, 2646, 2648, 2650, 2652, 2654, 2655, 2656, 2657, 2663, 2664, 2668, 2669, 2670, 2672, 2675, 2677, 2680, 2686, 2689, 2690, 2692, 2693, 2694, 2697, 2698, 2699, 2701, 2713, 2714, 2715, 2720, 2721, 2722, 2723, 2724, 2725, 2733, 2734, 2738, 2739, 2740, 2750, 2752, 2753, 2754, 2758, 2759, 2760, 2762, 2763, 2764, 2767, 2768, 2774, 2778, 2783, 2785, 2788, 2789, 2793, 2794, 2795, 2797, 2798, 2801, 2807, 2808, 2812, 2816, 2817, 2820, 2823, 2829, 2830, 2833, 2834, 2835, 2836, 2837, 2838, 2844, 2848, 2852, 2855, 2856, 2857, 2858, 2859, 2861, 2863, 2864, 2865, 2870, 2873, 2875, 2876, 2877, 2878, 2879, 2882, 2887, 2888, 2891, 2893, 2894, 2897, 2898, 2901, 2903, 2906, 2908, 2917, 2918, 2919, 2920, 2921, 2922, 2924, 2925, 2934, 2937, 2938, 2939, 2944, 2945, 2946, 2948, 2949, 2950, 2951, 2952, 2953, 2955, 2959, 2960, 2962, 2963, 2969, 2970, 2972, 2973, 2974, 2975, 2978, 2980, 2981, 2982, 2986, 2989, 2991, 2992, 2994, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3013, 3018, 3020, 3023, 3025, 3033, 3034, 3035, 3037, 3042, 3051, 3054, 3066, 3069, 3071, 3075, 3076, 3077, 3078, 3082, 3088, 3097, 3099, 3104, 3109, 3110, 3111, 3114, 3115, 3120, 3130, 3131, 3135, 3138, 3141, 3148, 3152, 3156, 3157, 3158, 3163, 3164, 3165, 3171, 3172, 3176, 3177, 3179, 3186, 3187, 3190, 3197, 3201, 3203, 3204, 3207, 3208, 3209, 3211, 3214, 3216, 3218, 3222, 3224, 3226, 3229, 3232, 3233, 3236, 3238, 3239, 3240, 3245, 3246, 3254, 3256, 3259, 3260, 3266, 3268, 3271, 3272, 3278, 3280, 3283, 3284, 3286, 3287, 3291, 3295, 3298, 3299, 3301, 3303, 3305, 3306, 3308, 3309, 3311, 3314, 3316, 3317, 3324, 3339, 3341, 3348, 3349, 3351, 3358, 3359, 3362, 3364, 3366, 3367, 3371, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3386, 3391, 3393, 3396, 3399, 3400, 3402, 3408, 3410, 3416, 3417, 3425, 3426, 3432, 3433, 3434, 3436, 3437, 3439, 3441, 3442, 3443, 3444, 3445, 3449, 3454, 3455, 3462, 3463, 3464, 3470, 3471, 3472, 3474, 3477, 3479, 3482, 3483, 3485, 3486, 3490, 3491, 3492, 3493, 3494, 3503, 3504, 3505, 3512, 3513, 3517, 3519, 3522, 3523, 3524, 3527, 3533, 3538, 3540, 3543, 3545, 3548, 3553, 3556, 3560, 3564, 3572, 3573, 3574, 3576, 3583, 3584, 3589, 3590, 3591, 3594, 3596, 3598, 3611, 3618, 3623, 3624, 3625, 3626, 3629, 3630, 3632, 3633, 3635, 3637, 3644, 3646, 3647, 3655, 3658, 3661, 3666, 3680, 3681, 3686, 3687, 3689, 3691, 3692, 3695, 3697, 3698, 3700, 3701, 3705, 3708, 3710, 3713, 3723, 3730, 3738, 3740, 3742, 3744, 3745, 3749, 3752, 3753, 3754, 3756, 3762, 3766, 3769, 3775, 3777, 3783, 3787, 3793, 3795, 3796, 3799, 3802, 3805, 3813, 3814, 3818, 3828, 3830, 3831, 3837, 3838, 3841, 3850, 3851, 3852, 3854, 3857, 3860, 3865, 3874, 3878, 3887, 3895, 3897, 3902, 3903, 3905, 3909, 3914, 3915, 3921, 3922, 3923, 3924, 3931, 3932, 3933, 3934, 3942, 3944, 3945, 3946, 3948, 3951, 3955, 3962, 3966, 3967, 3972, 3974, 3975, 3976, 3984, 3986, 3987, 3988, 3992, 3997, 3999, 4001, 4002, 4004, 4007, 4010, 4011, 4016, 4026, 4031, 4042, 4046, 4047, 4048, 4050, 4051, 4053, 4054, 4055, 4060, 4076, 4084, 4086, 4101, 4105, 4117, 4120, 4121, 4123, 4129, 4134, 4139, 4140, 4145, 4149, 4152, 4154, 4157, 4158, 4160, 4161, 4165, 4174, 4175, 4177, 4178, 4184, 4185, 4197, 4199, 4200, 4202, 4218, 4220, 4221, 4222, 4227, 4236, 4244, 4247, 4254, 4262, 4273, 4274, 4278, 4281, 4291, 4299, 4304, 4306, 4324, 4328, 4339, 4340, 4341, 4348, 4354, 4356, 4367, 4368, 4370, 4373, 4374, 4377, 4378, 4381, 4394, 4397, 4401, 4402, 4404, 4406, 4407, 4409, 4412, 4413, 4417, 4418, 4420, 4421, 4422, 4425, 4428, 4440, 4446, 4447, 4448, 4456, 4459, 4472, 4473, 4475, 4482, 4489, 4491, 4492, 4499, 4505, 4511, 4513, 4514, 4517, 4518, 4527, 4529, 4535, 4536, 4538, 4556, 4564, 4577, 4582, 4585, 4589, 4594, 4607, 4636, 4641, 4643, 4647, 4655, 4657, 4659, 4662, 4664, 4671, 4672, 4681, 4685, 4695, 4710, 4711, 4721, 4722, 4732, 4738, 4743, 4744, 4761, 4767, 4771, 4779, 4780, 4806, 4808, 4811, 4813, 4818, 4820, 4821, 4823, 4833, 4835, 4838, 4841, 4851, 4853, 4864, 4878, 4881, 4887, 4910, 4934, 4935, 4943, 4951, 4956, 4963, 4977, 4995, 5002, 5008, 5011, 5053, 5061, 5065, 5067, 5085, 5099, 5104, 5124, 5131, 5144, 5146, 5151, 5175, 5183, 5233, 5236, 5240, 5258, 5272, 5275, 5276, 5279, 5282, 5291, 5312, 5320, 5323, 5353, 5357, 5360, 5373, 5375, 5380, 5386, 5393, 5395, 5407, 5409, 5411, 5415, 5418, 5426, 5433, 5434, 5448, 5457, 5466, 5469, 5489, 5498, 5507, 5519, 5523, 5531, 5538, 5540, 5553, 5559, 5562, 5564, 5566, 5577, 5579, 5580, 5585, 5586, 5633, 5634, 5640, 5652, 5672, 5674, 5699, 5700, 5715, 5726, 5748, 5751, 5757, 5772, 5776, 5806, 5825, 5868, 5882, 5887, 5891, 5892, 5893, 5902, 5928, 5929, 5952, 5972, 5977, 6006, 6007, 6010, 6012, 6026, 6050, 6056, 6062, 6064, 6072, 6074, 6093]\n",
            "2424\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}